{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: timestamping does nothing in combination with -O. See the manual\n",
      "for details.\n",
      "\n",
      "--2025-01-25 22:12:30--  https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 978202 (955K) [application/zip]\n",
      "Saving to: ‘ml-latest-small.zip’\n",
      "\n",
      "ml-latest-small.zip 100%[===================>] 955.28K  1.65MB/s    in 0.6s    \n",
      "\n",
      "2025-01-25 22:12:32 (1.65 MB/s) - ‘ml-latest-small.zip’ saved [978202/978202]\n",
      "\n",
      "Archive:  ml-latest-small.zip\n",
      "  inflating: ml-latest-small/links.csv  \n",
      "  inflating: ml-latest-small/tags.csv  \n",
      "  inflating: ml-latest-small/ratings.csv  \n",
      "  inflating: ml-latest-small/README.txt  \n",
      "  inflating: ml-latest-small/movies.csv  \n"
     ]
    }
   ],
   "source": [
    "!wget -O ml-latest-small.zip -N https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "!unzip -o ml-latest-small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ther ratings.csv file into a pandas dataframe\n",
    "import pandas as pd\n",
    "ratings = pd.read_csv('ml-latest-small/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100836, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Representation\n",
    "* Since the data is sparse (most users have not rated most movies), we should represent the data in a sparse format to save memory and improve computational efficiency.\n",
    "* The sparse format typically represents the data as a user-item matrix, where rows correspond to users, columns correspond to movies, and the values are the ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "* UserID and MovieID are categorical features and should be encoded using one-hot encoding or LabelEncoding\n",
    "* Rating is the target variable (output) we want to predict\n",
    "* Timestamp can be dropped as it won't contain any information regarding the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ratings data:\n",
      "   userId  movieId  rating  timestamp  user  movie\n",
      "0       1        1     4.0  964982703     0      0\n",
      "1       1        3     4.0  964981247     0      2\n",
      "2       1        6     4.0  964982224     0      5\n",
      "3       1       47     5.0  964983815     0     43\n",
      "4       1       50     5.0  964982931     0     46\n"
     ]
    }
   ],
   "source": [
    "# Label encode the user and movie ids\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoders\n",
    "user_enc = LabelEncoder()\n",
    "movie_enc = LabelEncoder()\n",
    "\n",
    "# Fit and transform the training data\n",
    "ratings['user'] = user_enc.fit_transform(ratings['userId'].values)\n",
    "ratings['movie'] = movie_enc.fit_transform(ratings['movieId'].values)\n",
    "\n",
    "# Display the first few rows of the test data\n",
    "print(\"\\nRatings data:\")\n",
    "print(ratings.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610 9724\n",
      "(610, 9724)\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 100836 stored elements and shape (610, 9724)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t4.0\n",
      "  (0, 2)\t4.0\n",
      "  (0, 5)\t4.0\n",
      "  (0, 43)\t5.0\n",
      "  (0, 46)\t5.0\n",
      "  (0, 62)\t3.0\n",
      "  (0, 89)\t5.0\n",
      "  (0, 97)\t4.0\n",
      "  (0, 124)\t5.0\n",
      "  (0, 130)\t5.0\n",
      "  (0, 136)\t5.0\n",
      "  (0, 184)\t5.0\n",
      "  (0, 190)\t3.0\n",
      "  (0, 197)\t5.0\n",
      "  (0, 201)\t4.0\n",
      "  (0, 224)\t5.0\n",
      "  (0, 257)\t3.0\n",
      "  (0, 275)\t3.0\n",
      "  (0, 291)\t5.0\n",
      "  (0, 307)\t4.0\n",
      "  (0, 314)\t4.0\n",
      "  (0, 320)\t5.0\n",
      "  (0, 325)\t4.0\n",
      "  (0, 367)\t3.0\n",
      "  (0, 384)\t4.0\n",
      "  :\t:\n",
      "  (609, 9238)\t5.0\n",
      "  (609, 9246)\t4.5\n",
      "  (609, 9256)\t4.0\n",
      "  (609, 9268)\t5.0\n",
      "  (609, 9274)\t3.5\n",
      "  (609, 9279)\t3.5\n",
      "  (609, 9282)\t3.0\n",
      "  (609, 9288)\t3.0\n",
      "  (609, 9304)\t3.0\n",
      "  (609, 9307)\t2.5\n",
      "  (609, 9312)\t4.5\n",
      "  (609, 9317)\t3.0\n",
      "  (609, 9324)\t3.0\n",
      "  (609, 9339)\t4.0\n",
      "  (609, 9341)\t4.0\n",
      "  (609, 9348)\t3.5\n",
      "  (609, 9371)\t3.5\n",
      "  (609, 9372)\t3.5\n",
      "  (609, 9374)\t5.0\n",
      "  (609, 9415)\t4.0\n",
      "  (609, 9416)\t4.0\n",
      "  (609, 9443)\t5.0\n",
      "  (609, 9444)\t5.0\n",
      "  (609, 9445)\t5.0\n",
      "  (609, 9485)\t3.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "#create a sparse matrix of train data\n",
    "n_users = ratings['user'].max() + 1\n",
    "n_movies = ratings['movie'].max() + 1\n",
    "print(n_users, n_movies)\n",
    "\n",
    "sparse_matrix_train = csr_matrix((ratings['rating'], (ratings['user'], ratings['movie'])), shape=(n_users, n_movies))\n",
    "\n",
    "print(sparse_matrix_train.shape)\n",
    "print(sparse_matrix_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank of the users x movie sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def rank_calculator(sparse_matrix_train, energy):\n",
    "  \n",
    "  #Convert to array\n",
    "  train_data_matrix = sparse_matrix_train.toarray()\n",
    "\n",
    "  #Perform SVD\n",
    "  u, s, vt = svds(train_data_matrix, k=min(train_data_matrix.shape) - 1)\n",
    "\n",
    "  #Sort the s values in descending order\n",
    "  s = np.sort(s)[::-1]\n",
    "\n",
    "  #Calculate the total energy\n",
    "  total_energy = np.sum(s**2)\n",
    "  current_energy = 0\n",
    "  rank = 0\n",
    "  for i in range(len(s)):\n",
    "    current_energy += s[i]**2\n",
    "    if current_energy/total_energy >= energy:\n",
    "      rank = i\n",
    "      break\n",
    "    \n",
    "  print(\"Rank of the matrix with {energy} energy: \", rank)\n",
    "  return rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the error calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse\n",
    "\n",
    "# error calculation function for sparse matrix\n",
    "\n",
    "def error_calc(X_S, Y_S):\n",
    "\n",
    "    # Convert sparse matrices to dense arrays if necessary\n",
    "    if issparse(X_S):\n",
    "        X_S = X_S.toarray()\n",
    "    if issparse(Y_S):\n",
    "        Y_S = Y_S.toarray()\n",
    "    \n",
    "    #Norm calculation X_S - Y_S\n",
    "    normXminusY = np.linalg.norm(X_S - Y_S, ord=2)\n",
    "\n",
    "    #Norm of Y_S\n",
    "    norm_Y = np.linalg.norm(Y_S, ord=2)\n",
    "\n",
    "    if norm_Y == 0:\n",
    "        raise ValueError(\"Norm of Y is 0\")\n",
    "\n",
    "    #Calculate the error\n",
    "    error = normXminusY/norm_Y\n",
    "  \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Projection Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVP aims to find a low-rank matrix $X$ taht approximates an observed matrix $Y$ by solving:\n",
    "$$\n",
    "\\min_{X} \\|X - Y\\|_F^2 \\quad \\text{subject to} \\quad \\text{rank}(X) \\leq r\n",
    "$$\n",
    "where the rank $r$ is a fixed desired rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of the matrix with {energy} energy:  239\n"
     ]
    }
   ],
   "source": [
    "# Get the rank for 90% energy\n",
    "rank = rank_calculator(sparse_matrix_train, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def svp_algo(Y, rank, tol, max_iter = 1000):\n",
    "    #Init X\n",
    "    X = lil_matrix(Y.shape, dtype = np.float64) #Sparse matrix with 64 bit float values for better stability\n",
    "    iter = 0\n",
    "    error = tol + 99\n",
    "    prev_error = 100\n",
    "    rank = 239\n",
    "    while(error > tol and iter < max_iter and prev_error > error):\n",
    "        #Update the previous error\n",
    "        prev_error = error\n",
    "\n",
    "        #Update the iteration count\n",
    "        iter += 1\n",
    "\n",
    "        #Update the X matrix\n",
    "        X_half = X + (Y - X)\n",
    "        U, S, Vt = svds(X_half, k = rank)\n",
    "        X = U @ np.diag(S) @ Vt  #this is a dense matrix\n",
    "        #convert to sparse matrix\n",
    "        X = csr_matrix(X)\n",
    "\n",
    "        #Calculate the error\n",
    "        error = error_calc(X, Y)\n",
    "\n",
    "    if error <= tol:\n",
    "        print(f\"Converged after {iter} iterations with error {error:.6f}.\")\n",
    "    elif iter >= max_iter:\n",
    "        print(f\"Reached maximum iterations ({max_iter}) with final error {error:.6f}.\")\n",
    "    else:\n",
    "        print(f\"Stopped early due to increasing error. Final error: {error:.6f}.\")\n",
    "    \n",
    "    return iter, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped early due to increasing error. Final error: 0.058972.\n",
      "Time taken:  23.56357169151306\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Run the SVP algorithm\n",
    "iter, error_val_SVP = svp_algo(sparse_matrix_train, rank, 1e-5)\n",
    "\n",
    "time_svp = time.time() - start_time\n",
    "\n",
    "print(\"Time taken: \", time_svp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVP Estimator class to use with cross validation in scikit learn\n",
    "\n",
    "class SVPEstimator(BaseEstimator):\n",
    "    def __init__(self, rank=10, max_iter=100, tol=1e-3, learning_rate=1):\n",
    "        self.rank = rank\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.learning_rate = learning_rate\n",
    "        self.X = None\n",
    "        self.fit_time = None\n",
    "        self.error = None\n",
    "        self.iter = None\n",
    "        \n",
    "\n",
    "    def fit(self, Y, y=None):\n",
    "        start_time = time.time()\n",
    "        #Init X\n",
    "        self.X = lil_matrix(Y.shape, dtype = np.float64) #Sparse matrix with 64 bit float values for better stability\n",
    "        iter = 0\n",
    "        error = self.tol + 99\n",
    "        prev_error = 100\n",
    "        while(error > self.tol and iter < self.max_iter and prev_error > error):\n",
    "            #Update the previous error\n",
    "            prev_error = error\n",
    "\n",
    "            #Update the iteration count\n",
    "            iter += 1\n",
    "\n",
    "            #Update the X matrix\n",
    "            X_half = self.X + self.learning_rate * (Y - self.X)\n",
    "            U, S, Vt = svds(X_half, k = self.rank)\n",
    "            U_sparse = csr_matrix(U)\n",
    "            Vt_sparse = csr_matrix(Vt)\n",
    "            S_sparse = csr_matrix(np.diag(S))\n",
    "            self.X = U_sparse @ S_sparse @ Vt_sparse \n",
    "            \n",
    "            #Calculate the error\n",
    "            error = self._error_calc(self.X, Y)\n",
    "\n",
    "        self.error = error\n",
    "        self.iter = iter\n",
    "        self.fit_time = time.time() - start_time\n",
    "        print(f\"Fit time: {self.fit_time:.6f} seconds with {self.iter} iterations and error {self.error:.6f}.\")\n",
    "        return self\n",
    "    \n",
    "    def score(self, Y, y=None):\n",
    "        # Extract observed entries in the validation fold\n",
    "        val_indices = list(zip(Y.nonzero()[0], Y.nonzero()[1]))\n",
    "        val_values = Y.data\n",
    "\n",
    "        # Compute the error only on the observed entries in the validation fold\n",
    "        error = self._error_calc_validation(self.X, val_indices, val_values)\n",
    "        return -error  # For scikit-learn, higher score is better\n",
    "\n",
    "    def _error_calc(self, X_S, Y_S):\n",
    "\n",
    "        # Convert sparse matrices to dense arrays if necessary\n",
    "        if issparse(X_S):\n",
    "            X_S = X_S.toarray()\n",
    "        if issparse(Y_S):\n",
    "            Y_S = Y_S.toarray()\n",
    "        \n",
    "        #Norm calculation X_S - Y_S\n",
    "        normXminusY = np.linalg.norm(X_S - Y_S, ord=2)\n",
    "\n",
    "        #Norm of Y_S\n",
    "        norm_Y = np.linalg.norm(Y_S, ord=2)\n",
    "\n",
    "        if norm_Y == 0:\n",
    "            raise ValueError(\"Norm of Y is 0\")\n",
    "\n",
    "        #Calculate the error\n",
    "        error = normXminusY/norm_Y\n",
    "    \n",
    "        return error\n",
    "\n",
    "    def _error_calc_validation(self, X, val_indices, val_values):\n",
    "        \n",
    "        # Extract predicted values at the validation indices\n",
    "        pred_values = np.array([X[i, j] for (i, j) in val_indices])\n",
    "\n",
    "        # Compute the Frobenius norm of the difference\n",
    "        numerator = np.linalg.norm(pred_values - val_values, ord=2)\n",
    "\n",
    "        # Compute the Frobenius norm of the validation values\n",
    "        denominator = np.linalg.norm(val_values, ord=2)\n",
    "\n",
    "        if denominator == 0:\n",
    "            raise ValueError(\"Norm of validation values is 0\")\n",
    "\n",
    "        # Compute the relative error\n",
    "        error = numerator / denominator\n",
    "\n",
    "        return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of the matrix with {energy} energy:  239\n"
     ]
    }
   ],
   "source": [
    "# Get the rank for 90% energy\n",
    "rank = rank_calculator(sparse_matrix_train, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank:  239\n"
     ]
    }
   ],
   "source": [
    "print(\"Rank: \", rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 7.723691 seconds with 2 iterations and error 0.052819.\n",
      "Fit time: 14.856568 seconds with 3 iterations and error 0.054237.\n",
      "Fit time: 11.062369 seconds with 3 iterations and error 0.055145.\n",
      "Fit time: 12.055509 seconds with 3 iterations and error 0.057840.\n",
      "Fit time: 11.274063 seconds with 2 iterations and error 0.055710.\n",
      "Cross validation errors:  [0.95114829 0.9419772  0.95355127 0.95305303 0.95217684]\n",
      "Mean CV error:  0.950381327654785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#Initialize the SVP Estimator\n",
    "svp_estimator = SVPEstimator(rank=rank, max_iter=100, tol=1e-3, learning_rate=1)\n",
    "\n",
    "#Perform 5 fold CV\n",
    "cv_scores = cross_val_score(svp_estimator, sparse_matrix_train, cv=5)\n",
    "\n",
    "print(\"Cross validation errors: \", -cv_scores)\n",
    "print(\"Mean CV error: \", -np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve SVP\n",
    "* Lets try to improve the algorithm by specifying a learning rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 157.659040 seconds with 27 iterations and error 0.079024.\n",
      "Fit time: 172.767256 seconds with 25 iterations and error 0.080419.\n",
      "Fit time: 147.803936 seconds with 25 iterations and error 0.081296.\n",
      "Fit time: 148.888776 seconds with 25 iterations and error 0.085676.\n",
      "Fit time: 144.683238 seconds with 25 iterations and error 0.082050.\n",
      "Cross validation errors:  [0.94512509 0.93814812 0.95096044 0.94950286 0.94870988]\n",
      "Mean CV error:  0.9464892794882342\n"
     ]
    }
   ],
   "source": [
    "#Initialize the SVP Estimator\n",
    "svp_estimator = SVPEstimator(rank=rank, max_iter=100, tol=1e-3, learning_rate=0.1)\n",
    "\n",
    "#Perform 5 fold CV\n",
    "cv_scores = cross_val_score(svp_estimator, sparse_matrix_train, cv=5)\n",
    "\n",
    "print(\"Cross validation errors: \", -cv_scores)\n",
    "print(\"Mean CV error: \", -np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion SVP\n",
    "* The SVP algorithm converges quite fast since the rank is fixed. (Hard rank constraint)\n",
    "* There seems to be over-fitting happening looking at the cross validation error scores. This means that it is capturing the noise in the training folds. Probably the rank of the matrix is chosen to be too high.\n",
    "* This SVP method does not have regularization. Maybe with regularization the results could be better, as we can penelize large values. (As we will see in the next algorithm)\n",
    "* Increasing the data could also yield better results.\n",
    "*  By adding the learning rate parameter of 0.1 the results are identical, which is expected, since in all cases the maximum number of iterations is not acheived, and we are at the same minimum point on the convex curve.\n",
    "* The SVP algorithm is best suited for data that has less noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVT introduces a soft thresholding of the singular values which brings about a nuclear norm regularization.\n",
    "$$\n",
    "\\min_{X} \\|X - Y\\|_F^2 + \\lambda \\|X\\|_*\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVP Estimator class to use with cross validation in scikit learn\n",
    "\n",
    "class SVTEstimator(BaseEstimator):\n",
    "    def __init__(self, rank=10, max_iter=100, tol=1e-3, learning_rate=1, lambda_=0):\n",
    "        self.rank = rank\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_ = lambda_\n",
    "        self.X = None\n",
    "        self.fit_time = None\n",
    "        self.error = None\n",
    "        self.iter = None\n",
    "        \n",
    "\n",
    "    def fit(self, Y, y=None):\n",
    "        start_time = time.time()\n",
    "        #Init X\n",
    "        self.X = lil_matrix(Y.shape, dtype = np.float64) #Sparse matrix with 64 bit float values for better stability\n",
    "        iter = 0\n",
    "        error = self.tol + 99\n",
    "        prev_error = 100\n",
    "        while(error > self.tol and iter < self.max_iter and prev_error > error):\n",
    "            #Update the previous error\n",
    "            prev_error = error\n",
    "\n",
    "            #Update the iteration count\n",
    "            iter += 1\n",
    "\n",
    "            #Update the X matrix\n",
    "            X_half = self.X + self.learning_rate * (Y - self.X)\n",
    "            U, S, Vt = svds(X_half, k = min(X_half.shape) - 1) # Take full possible rank\n",
    "            S = np.maximum(S - self.lambda_, 0) #Soft thresholding\n",
    "            U_sparse = csr_matrix(U)\n",
    "            Vt_sparse = csr_matrix(Vt)\n",
    "            S_sparse = csr_matrix(np.diag(S))\n",
    "            self.X = U_sparse @ S_sparse @ Vt_sparse \n",
    "\n",
    "            #Calculate the error\n",
    "            error = self._error_calc(self.X, Y)\n",
    "\n",
    "        self.error = error\n",
    "        self.iter = iter\n",
    "        self.fit_time = time.time() - start_time\n",
    "        print(f\"Fit time: {self.fit_time:.6f} seconds with {self.iter} iterations and error {self.error:.6f}.\")\n",
    "        return self\n",
    "    \n",
    "    def score(self, Y, y=None):\n",
    "        # Extract observed entries in the validation fold\n",
    "        val_indices = list(zip(Y.nonzero()[0], Y.nonzero()[1]))\n",
    "        val_values = Y.data\n",
    "\n",
    "        # Compute the error only on the observed entries in the validation fold\n",
    "        error = self._error_calc_validation(self.X, val_indices, val_values)\n",
    "        return -error  # For scikit-learn, higher score is better\n",
    "\n",
    "    def _error_calc(self, X_S, Y_S):\n",
    "\n",
    "        # Convert sparse matrices to dense arrays if necessary\n",
    "        if issparse(X_S):\n",
    "            X_S = X_S.toarray()\n",
    "        if issparse(Y_S):\n",
    "            Y_S = Y_S.toarray()\n",
    "        \n",
    "        #Norm calculation X_S - Y_S\n",
    "        normXminusY = np.linalg.norm(X_S - Y_S, ord=2)\n",
    "\n",
    "        #Norm of Y_S\n",
    "        norm_Y = np.linalg.norm(Y_S, ord=2)\n",
    "\n",
    "        if norm_Y == 0:\n",
    "            raise ValueError(\"Norm of Y is 0\")\n",
    "\n",
    "        #Calculate the error\n",
    "        error = normXminusY/norm_Y\n",
    "    \n",
    "        return error\n",
    "\n",
    "    def _error_calc_validation(self, X, val_indices, val_values):\n",
    "        \n",
    "        # Extract predicted values at the validation indices\n",
    "        pred_values = np.array([X[i, j] for (i, j) in val_indices])\n",
    "\n",
    "        # Compute the Frobenius norm of the difference\n",
    "        numerator = np.linalg.norm(pred_values - val_values, ord=2)\n",
    "\n",
    "        # Compute the Frobenius norm of the validation values\n",
    "        denominator = np.linalg.norm(val_values, ord=2)\n",
    "\n",
    "        if denominator == 0:\n",
    "            raise ValueError(\"Norm of validation values is 0\")\n",
    "\n",
    "        # Compute the relative error\n",
    "        error = numerator / denominator\n",
    "\n",
    "        return error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of hyper parameter $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to select the correct values for the hyper parameter lambda.\n",
    "* $\\lambda$ determines the threshold for the singular values\n",
    "* Larger $\\lambda$ values shrink more singular values towards zero, resulting in lower-rank matrices (under-fitting)\n",
    "* Smaller $\\lambda$ values do the opposite, resulting in better fit for the training data (over-fitting)\n",
    "* Using the right $\\lambda$ we can control the over/under fitting of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 26.970989 seconds with 4 iterations and error 0.007786.\n",
      "Fit time: 14.028149 seconds with 2 iterations and error 0.008035.\n",
      "Fit time: 12.672446 seconds with 2 iterations and error 0.007583.\n",
      "Fit time: 31.020789 seconds with 4 iterations and error 0.007186.\n",
      "Fit time: 22.276126 seconds with 3 iterations and error 0.008388.\n",
      "Fit time: 26.519989 seconds with 4 iterations and error 0.007786.\n",
      "Fit time: 18.045598 seconds with 3 iterations and error 0.008035.\n",
      "Fit time: 30.127150 seconds with 4 iterations and error 0.007583.\n",
      "Fit time: 12.269519 seconds with 2 iterations and error 0.007186.\n",
      "Fit time: 15.692837 seconds with 2 iterations and error 0.008388.\n",
      "Fit time: 26.949672 seconds with 4 iterations and error 0.007786.\n",
      "Fit time: 20.221341 seconds with 3 iterations and error 0.008035.\n",
      "Fit time: 16.054317 seconds with 2 iterations and error 0.007583.\n",
      "Fit time: 23.999231 seconds with 4 iterations and error 0.007186.\n",
      "Fit time: 26.800091 seconds with 3 iterations and error 0.008388.\n",
      "Fit time: 11.461674 seconds with 2 iterations and error 0.007786.\n",
      "Fit time: 25.994732 seconds with 3 iterations and error 0.008035.\n",
      "Fit time: 23.528373 seconds with 3 iterations and error 0.007583.\n",
      "Fit time: 25.492945 seconds with 3 iterations and error 0.007186.\n",
      "Fit time: 12.476680 seconds with 2 iterations and error 0.008388.\n",
      "Fit time: 17.006300 seconds with 3 iterations and error 0.022390.\n",
      "Fit time: 18.398451 seconds with 3 iterations and error 0.020289.\n",
      "Fit time: 12.855705 seconds with 2 iterations and error 0.020289.\n",
      "Fit time: 19.203791 seconds with 3 iterations and error 0.020318.\n",
      "Fit time: 16.349446 seconds with 2 iterations and error 0.021235.\n",
      "Best lambda: 10.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define a range for lambda\n",
    "lambda_values = np.logspace(-3, 1, 5)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_errors = []\n",
    "for lambda_ in lambda_values:\n",
    "    svt_estimator = SVTEstimator(lambda_=lambda_, learning_rate=1, max_iter=100, tol=1e-6)\n",
    "    cv_scores = cross_val_score(svt_estimator, sparse_matrix_train, cv=KFold(n_splits=5, shuffle=True, random_state=42))\n",
    "    cv_errors.append(-np.mean(cv_scores))  # Convert scores to errors\n",
    "\n",
    "# Find the best lambda\n",
    "best_lambda = lambda_values[np.argmin(cv_errors)]\n",
    "print(f\"Best lambda: {best_lambda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion SVT\n",
    "* SVT provides soft thresholding to the singular values\n",
    "* This in essence is the nuclear norm regularization\n",
    "* It is more robust to noise\n",
    "* Convergense speed is much slower\n",
    "* Useful when data is noisy\n",
    "* Lambda parameter tuning is very important"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLlabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
