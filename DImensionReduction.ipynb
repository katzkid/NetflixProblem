{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Netflix Recommendation Problem - Dimension Reduction Techniques\n",
    "## Authors: Ba Khuong DANG, Kartik VISWANATHAN\n",
    "## MSD 2024-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: timestamping does nothing in combination with -O. See the manual\n",
      "for details.\n",
      "\n",
      "--2025-02-12 13:21:51--  https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 978202 (955K) [application/zip]\n",
      "Saving to: ‘ml-latest-small.zip’\n",
      "\n",
      "ml-latest-small.zip 100%[===================>] 955.28K  1.49MB/s    in 0.6s    \n",
      "\n",
      "2025-02-12 13:21:52 (1.49 MB/s) - ‘ml-latest-small.zip’ saved [978202/978202]\n",
      "\n",
      "Archive:  ml-latest-small.zip\n",
      "  inflating: ml-latest-small/links.csv  \n",
      "  inflating: ml-latest-small/tags.csv  \n",
      "  inflating: ml-latest-small/ratings.csv  \n",
      "  inflating: ml-latest-small/README.txt  \n",
      "  inflating: ml-latest-small/movies.csv  \n"
     ]
    }
   ],
   "source": [
    "!wget -O ml-latest-small.zip -N https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "!unzip -o ml-latest-small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ther ratings.csv file into a pandas dataframe\n",
    "import pandas as pd\n",
    "ratings = pd.read_csv('ml-latest-small/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100836, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Representation\n",
    "* Since the data is sparse (most users have not rated most movies), we should represent the data in a sparse format to save memory and improve computational efficiency.\n",
    "* The sparse format typically represents the data as a user-item matrix, where rows correspond to users, columns correspond to movies, and the values are the ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "* UserID and MovieID are categorical features and should be encoded using one-hot encoding or LabelEncoding\n",
    "* Rating is the target variable (output) we want to predict\n",
    "* Timestamp can be dropped as it won't contain any information regarding the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ratings data:\n",
      "   userId  movieId  rating  timestamp  user  movie\n",
      "0       1        1     4.0  964982703     0      0\n",
      "1       1        3     4.0  964981247     0      2\n",
      "2       1        6     4.0  964982224     0      5\n",
      "3       1       47     5.0  964983815     0     43\n",
      "4       1       50     5.0  964982931     0     46\n"
     ]
    }
   ],
   "source": [
    "# Label encode the user and movie ids\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoders\n",
    "user_enc = LabelEncoder()\n",
    "movie_enc = LabelEncoder()\n",
    "\n",
    "# Fit and transform the training data\n",
    "ratings['user'] = user_enc.fit_transform(ratings['userId'].values)\n",
    "ratings['movie'] = movie_enc.fit_transform(ratings['movieId'].values)\n",
    "\n",
    "# Display the first few rows of the test data\n",
    "print(\"\\nRatings data:\")\n",
    "print(ratings.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610 9724\n",
      "(610, 9724)\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 100836 stored elements and shape (610, 9724)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t4.0\n",
      "  (0, 2)\t4.0\n",
      "  (0, 5)\t4.0\n",
      "  (0, 43)\t5.0\n",
      "  (0, 46)\t5.0\n",
      "  (0, 62)\t3.0\n",
      "  (0, 89)\t5.0\n",
      "  (0, 97)\t4.0\n",
      "  (0, 124)\t5.0\n",
      "  (0, 130)\t5.0\n",
      "  (0, 136)\t5.0\n",
      "  (0, 184)\t5.0\n",
      "  (0, 190)\t3.0\n",
      "  (0, 197)\t5.0\n",
      "  (0, 201)\t4.0\n",
      "  (0, 224)\t5.0\n",
      "  (0, 257)\t3.0\n",
      "  (0, 275)\t3.0\n",
      "  (0, 291)\t5.0\n",
      "  (0, 307)\t4.0\n",
      "  (0, 314)\t4.0\n",
      "  (0, 320)\t5.0\n",
      "  (0, 325)\t4.0\n",
      "  (0, 367)\t3.0\n",
      "  (0, 384)\t4.0\n",
      "  :\t:\n",
      "  (609, 9238)\t5.0\n",
      "  (609, 9246)\t4.5\n",
      "  (609, 9256)\t4.0\n",
      "  (609, 9268)\t5.0\n",
      "  (609, 9274)\t3.5\n",
      "  (609, 9279)\t3.5\n",
      "  (609, 9282)\t3.0\n",
      "  (609, 9288)\t3.0\n",
      "  (609, 9304)\t3.0\n",
      "  (609, 9307)\t2.5\n",
      "  (609, 9312)\t4.5\n",
      "  (609, 9317)\t3.0\n",
      "  (609, 9324)\t3.0\n",
      "  (609, 9339)\t4.0\n",
      "  (609, 9341)\t4.0\n",
      "  (609, 9348)\t3.5\n",
      "  (609, 9371)\t3.5\n",
      "  (609, 9372)\t3.5\n",
      "  (609, 9374)\t5.0\n",
      "  (609, 9415)\t4.0\n",
      "  (609, 9416)\t4.0\n",
      "  (609, 9443)\t5.0\n",
      "  (609, 9444)\t5.0\n",
      "  (609, 9445)\t5.0\n",
      "  (609, 9485)\t3.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "#create a sparse matrix of train data\n",
    "n_users = ratings['user'].max() + 1\n",
    "n_movies = ratings['movie'].max() + 1\n",
    "print(n_users, n_movies)\n",
    "\n",
    "sparse_matrix_train = csr_matrix((ratings['rating'], (ratings['user'], ratings['movie'])), shape=(n_users, n_movies))\n",
    "\n",
    "print(sparse_matrix_train.shape)\n",
    "print(sparse_matrix_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank of the users x movie sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank is calculated based on $\\epsilon$ which is a fixed parameter that determines the level at which singular values are \"shrunk\" towards zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def rank_calculator(sparse_matrix_train, epsilon):\n",
    "  \n",
    "  #Convert to array\n",
    "  train_data_matrix = sparse_matrix_train.toarray()\n",
    "\n",
    "  #Perform SVD\n",
    "  u, s, vt = svds(train_data_matrix, k=min(train_data_matrix.shape) - 1)\n",
    "\n",
    "  #Sort the s values in descending order\n",
    "  s = np.sort(s)[::-1]\n",
    "\n",
    "  for i in range(len(s)-1):\n",
    "    if s[i] - s[i+1] < epsilon:\n",
    "      tau = s[i]\n",
    "      rank = i + 1\n",
    "      break\n",
    "\n",
    "  total_energy = np.sum(s**2)  # Calculate total energy\n",
    "  current_energy = np.sum(s[:rank]**2)  # Calculate energy based on the correct rank\n",
    "  energy = current_energy / total_energy  # Calculate the energy based on the correct rank\n",
    "\n",
    "\n",
    "  return rank, energy, tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the error calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse\n",
    "\n",
    "# error calculation function for sparse matrix\n",
    "\n",
    "def error_calc(X_S, Y_S):\n",
    "\n",
    "    # Convert sparse matrices to dense arrays if necessary\n",
    "    if issparse(X_S):\n",
    "        X_S = X_S.toarray()\n",
    "    if issparse(Y_S):\n",
    "        Y_S = Y_S.toarray()\n",
    "    \n",
    "    #Norm calculation X_S - Y_S\n",
    "    normXminusY = np.linalg.norm(X_S - Y_S, ord=2)\n",
    "\n",
    "    #Norm of Y_S\n",
    "    norm_Y = np.linalg.norm(Y_S, ord=2)\n",
    "\n",
    "    if norm_Y == 0:\n",
    "        raise ValueError(\"Norm of Y is 0\")\n",
    "\n",
    "    #Calculate the error\n",
    "    error = normXminusY/norm_Y\n",
    "  \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Projection Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVP aims to find a low-rank matrix $X$ that approximates an observed matrix $Y$ by solving:\n",
    "$$\n",
    "\\min_{X} \\|X - Y\\|_F^2 \\quad \\text{subject to} \\quad \\text{rank}(X) \\leq r\n",
    "$$\n",
    "where the rank $r$ is a fixed desired rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank= 216 energy= 0.8809382042360632 tau= 34.276247085610635\n"
     ]
    }
   ],
   "source": [
    "# Get the rank for 90% energy\n",
    "rank, energy, tau = rank_calculator(sparse_matrix_train, 0.002)\n",
    "print(\"rank=\", rank, \"energy=\", energy, \"tau=\", tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.base import BaseEstimator\n",
    "import time\n",
    "from scipy.sparse import issparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVP Estimator class to use with cross validation in scikit learn\n",
    "\n",
    "class SVPEstimator(BaseEstimator):\n",
    "    def __init__(self, rank=10, max_iter=100, tol=1e-3, learning_rate=1):\n",
    "        self.rank = rank\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.learning_rate = learning_rate\n",
    "        self.X = None\n",
    "        self.fit_time = None\n",
    "        self.error = None\n",
    "        self.iter = None\n",
    "        \n",
    "\n",
    "    def fit(self, Y, y=None):\n",
    "        start_time = time.time()\n",
    "        #Init X\n",
    "        self.X = lil_matrix(Y.shape, dtype = np.float64) #Sparse matrix with 64 bit float values for better stability\n",
    "        iter = 0\n",
    "        error = self.tol + 99\n",
    "        prev_error = 100\n",
    "        while(error > self.tol and iter < self.max_iter and prev_error > error):\n",
    "            #Update the previous error\n",
    "            prev_error = error\n",
    "\n",
    "            #Update the iteration count\n",
    "            iter += 1\n",
    "\n",
    "            #Update the X matrix\n",
    "            X_half = self.X + self.learning_rate * (Y - self.X)\n",
    "            U, S, Vt = svds(X_half, k = self.rank)\n",
    "            U_sparse = csr_matrix(U)\n",
    "            Vt_sparse = csr_matrix(Vt)\n",
    "            S_sparse = csr_matrix(np.diag(S))\n",
    "            self.X = U_sparse @ S_sparse @ Vt_sparse \n",
    "            \n",
    "            #Calculate the error\n",
    "            error = self._error_calc(self.X, Y)\n",
    "\n",
    "        self.error = error\n",
    "        self.iter = iter\n",
    "        self.fit_time = time.time() - start_time\n",
    "        print(f\"Fit time: {self.fit_time:.6f} seconds with {self.iter} iterations and error {self.error:.6f}.\")\n",
    "        return self\n",
    "    \n",
    "    def score(self, Y, y=None):\n",
    "        # Extract observed entries in the validation fold\n",
    "        val_indices = list(zip(Y.nonzero()[0], Y.nonzero()[1]))\n",
    "        val_values = Y.data\n",
    "\n",
    "        # Compute the error only on the observed entries in the validation fold\n",
    "        error = self._error_calc_validation(self.X, val_indices, val_values)\n",
    "        return -error  # For scikit-learn, higher score is better\n",
    "\n",
    "    def _error_calc(self, X_S, Y_S):\n",
    "\n",
    "        # Convert sparse matrices to dense arrays if necessary\n",
    "        if issparse(X_S):\n",
    "            X_S = X_S.toarray()\n",
    "        if issparse(Y_S):\n",
    "            Y_S = Y_S.toarray()\n",
    "        \n",
    "        #Norm calculation X_S - Y_S\n",
    "        normXminusY = np.linalg.norm(X_S - Y_S, ord=2)\n",
    "\n",
    "        #Norm of Y_S\n",
    "        norm_Y = np.linalg.norm(Y_S, ord=2)\n",
    "\n",
    "        if norm_Y == 0:\n",
    "            raise ValueError(\"Norm of Y is 0\")\n",
    "\n",
    "        #Calculate the error\n",
    "        error = normXminusY/norm_Y\n",
    "    \n",
    "        return error\n",
    "\n",
    "    def _error_calc_validation(self, X, val_indices, val_values):\n",
    "        \n",
    "        # Extract predicted values at the validation indices\n",
    "        pred_values = np.array([X[i, j] for (i, j) in val_indices])\n",
    "\n",
    "        # Compute the Frobenius norm of the difference\n",
    "        numerator = np.linalg.norm(pred_values - val_values, ord=2)\n",
    "\n",
    "        # Compute the Frobenius norm of the validation values\n",
    "        denominator = np.linalg.norm(val_values, ord=2)\n",
    "\n",
    "        if denominator == 0:\n",
    "            raise ValueError(\"Norm of validation values is 0\")\n",
    "\n",
    "        # Compute the relative error\n",
    "        error = numerator / denominator\n",
    "\n",
    "        return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 18.471084 seconds with 3 iterations and error 0.064134.\n",
      "Fit time SVP One iteration:  6.157027959823608\n",
      "Fit time: 9.495206 seconds with 2 iterations and error 0.060736.\n",
      "Fit time: 9.621317 seconds with 2 iterations and error 0.063242.\n",
      "Fit time: 9.919137 seconds with 2 iterations and error 0.058039.\n",
      "Fit time: 9.946097 seconds with 2 iterations and error 0.060150.\n",
      "Fit time: 10.099894 seconds with 2 iterations and error 0.059354.\n",
      "Cross validation errors:  [0.949641   0.94135507 0.95269472 0.95222441 0.95165312]\n",
      "Mean CV error:  0.9495136629343819\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#Initialize the SVP Estimator\n",
    "svp_estimator = SVPEstimator(rank=rank, max_iter=100, tol=1e-3, learning_rate=1)\n",
    "\n",
    "#fit the model once for timing purposes\n",
    "svp_estimator.fit(sparse_matrix_train)\n",
    "SVPTimingOneIter = svp_estimator.fit_time/svp_estimator.iter\n",
    "print(\"Fit time SVP One iteration: \", SVPTimingOneIter)\n",
    "\n",
    "#Perform 5 fold CV\n",
    "cv_scores = cross_val_score(svp_estimator, sparse_matrix_train, cv=5, n_jobs=10)\n",
    "\n",
    "print(\"Cross validation errors: \", -cv_scores)\n",
    "print(\"Mean CV error: \", -np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve SVP based on learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 526.345350 seconds with 35 iterations and error 0.062799.\n",
      "Fit time: 567.554138 seconds with 36 iterations and error 0.059501.\n",
      "Fit time: 572.897774 seconds with 35 iterations and error 0.059296.\n",
      "Fit time: 576.597844 seconds with 36 iterations and error 0.061912.\n",
      "Fit time: 579.490688 seconds with 37 iterations and error 0.058775.\n",
      "Cross validation errors:  [0.95945764 0.95602964 0.93400425 0.95142328 0.95331217]\n",
      "Mean CV error:  0.950845395702809\n",
      "Fit time: 434.294079 seconds with 29 iterations and error 0.062799.\n",
      "Fit time: 446.902071 seconds with 28 iterations and error 0.061912.\n",
      "Fit time: 449.548540 seconds with 28 iterations and error 0.058775.\n",
      "Fit time: 456.115894 seconds with 29 iterations and error 0.059501.\n",
      "Fit time: 456.949611 seconds with 28 iterations and error 0.059296.\n",
      "Cross validation errors:  [0.9594225  0.95601807 0.9339909  0.95146222 0.95333579]\n",
      "Mean CV error:  0.9508458980906127\n",
      "Fit time: 221.946574 seconds with 14 iterations and error 0.058775.\n",
      "Fit time: 222.054298 seconds with 14 iterations and error 0.059501.\n",
      "Fit time: 224.272121 seconds with 15 iterations and error 0.062799.\n",
      "Fit time: 223.255938 seconds with 14 iterations and error 0.061912.\n",
      "Fit time: 238.799693 seconds with 15 iterations and error 0.059296.\n",
      "Cross validation errors:  [0.95937111 0.95600498 0.93387714 0.95141372 0.95326294]\n",
      "Mean CV error:  0.9507859769428497\n",
      "Fit time: 84.066935 seconds with 6 iterations and error 0.062799.\n",
      "Fit time: 87.734444 seconds with 6 iterations and error 0.058775.\n",
      "Fit time: 88.886951 seconds with 6 iterations and error 0.059501.\n",
      "Fit time: 89.184568 seconds with 6 iterations and error 0.061912.\n",
      "Fit time: 99.385169 seconds with 7 iterations and error 0.059296.\n",
      "Cross validation errors:  [0.95931176 0.95592622 0.93377827 0.95129993 0.95306107]\n",
      "Mean CV error:  0.9506754503122321\n",
      "Fit time: 10.290928 seconds with 2 iterations and error 0.062799.\n",
      "Fit time: 10.777226 seconds with 2 iterations and error 0.059501.\n",
      "Fit time: 14.952233 seconds with 3 iterations and error 0.061912.\n",
      "Fit time: 15.393629 seconds with 3 iterations and error 0.058775.\n",
      "Fit time: 19.412383 seconds with 4 iterations and error 0.059296.\n",
      "Cross validation errors:  [0.95928666 0.95591375 0.93376871 0.95127082 0.95297981]\n",
      "Mean CV error:  0.9506439513148525\n",
      "Best leraning rate: 1\n",
      "Best error: 0.9506439513148525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# Define a range for learning rates\n",
    "learning_rates = [0.08, 0.1, 0.2, 0.5, 1]\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_errors = []\n",
    "for learning_rate in learning_rates:\n",
    "    svp_estimator = SVPEstimator(rank=rank, learning_rate=learning_rate, max_iter=100, tol=1e-3)\n",
    "    cv_scores = cross_val_score(svp_estimator, sparse_matrix_train, cv=KFold(n_splits=5, shuffle=True, random_state=42), n_jobs=10)\n",
    "    cv_errors.append(-np.mean(cv_scores))  # Convert scores to errors\n",
    "    print(\"Cross validation errors: \", -cv_scores)\n",
    "    print(\"Mean CV error: \", -np.mean(cv_scores))\n",
    "\n",
    "# Find the best lambda\n",
    "best_learning_rate = learning_rates[np.argmin(cv_errors)]\n",
    "print(f\"Best leraning rate: {learning_rate}\")\n",
    "\n",
    "# Best error\n",
    "best_error_svp = np.min(cv_errors)\n",
    "print(f\"Best error: {best_error_svp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion SVP\n",
    "* The SVP algorithm converges quite fast since the rank is fixed. (Hard rank constraint)\n",
    "* There seems to be over-fitting happening looking at the cross validation error scores. This means that it is capturing the noise in the training folds. Probably the rank of the matrix is chosen to be too high.\n",
    "* This SVP method does not have regularization. Maybe with regularization the results could be better, as we can penelize large values. (As we will see in the next algorithm)\n",
    "* Increasing the data could also yield better results.\n",
    "* Learning rate of 1 has the best error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVT introduces a soft thresholding of the singular values which brings about a nuclear norm regularization.\n",
    "$$\n",
    "\\min_{X} \\|X - Y\\|_F^2 + \\lambda \\|X\\|_*\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of hyper parameter $\\tau$\n",
    "It is very important to select the correct values for the hyper parameter $\\tau$.\n",
    "* $\\tau$ determines the threshold for the singular values\n",
    "* Larger $\\tau$ values shrink more singular values towards zero, resulting in lower-rank matrices (under-fitting)\n",
    "* Smaller $\\tau$ values do the opposite, resulting in better fit for the training data (over-fitting)\n",
    "* Using the right $\\tau$ we can control the over/under fitting of the data\n",
    "\n",
    "In our case we have selected $\\tau = 0.02$ in order to have around $90\\%$ of energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVP Estimator class to use with cross validation in scikit learn\n",
    "\n",
    "class SVTEstimator(BaseEstimator):\n",
    "    def __init__(self, max_iter=100, tol=1e-3, learning_rate=1, tau=0):\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.X = None\n",
    "        self.fit_time = None\n",
    "        self.error = None\n",
    "        self.iter = None\n",
    "        \n",
    "\n",
    "    def fit(self, Y, y=None):\n",
    "        start_time = time.time()\n",
    "        #Init X\n",
    "        self.X = lil_matrix(Y.shape, dtype = np.float64) #Sparse matrix with 64 bit float values for better stability\n",
    "        iter = 0\n",
    "        error = self.tol + 99\n",
    "        prev_error = 100\n",
    "        while(error > self.tol and iter < self.max_iter and prev_error > error):\n",
    "            #Update the previous error\n",
    "            prev_error = error\n",
    "\n",
    "            #Update the iteration count\n",
    "            iter += 1\n",
    "\n",
    "            #Update the X matrix\n",
    "            X_half = self.X + self.learning_rate * (Y - self.X)\n",
    "            U, S, Vt = svds(X_half, k = min(X_half.shape) - 1) # Take full possible rank\n",
    "            S = np.maximum(S - self.tau, 0) #Soft thresholding according to the paper\n",
    "            U_sparse = csr_matrix(U)\n",
    "            Vt_sparse = csr_matrix(Vt)\n",
    "            S_sparse = csr_matrix(np.diag(S))\n",
    "            self.X = U_sparse @ S_sparse @ Vt_sparse \n",
    "\n",
    "            #Calculate the error\n",
    "            error = self._error_calc(self.X, Y)\n",
    "\n",
    "        self.error = error\n",
    "        self.iter = iter\n",
    "        self.fit_time = time.time() - start_time\n",
    "        print(f\"Fit time: {self.fit_time:.6f} seconds with {self.iter} iterations and error {self.error:.6f}.\")\n",
    "        return self\n",
    "    \n",
    "    def score(self, Y, y=None):\n",
    "        # Extract observed entries in the validation fold\n",
    "        val_indices = list(zip(Y.nonzero()[0], Y.nonzero()[1]))\n",
    "        val_values = Y.data\n",
    "\n",
    "        # Compute the error only on the observed entries in the validation fold\n",
    "        error = self._error_calc_validation(self.X, val_indices, val_values)\n",
    "        return -error  # For scikit-learn, higher score is better\n",
    "\n",
    "    def _error_calc(self, X_S, Y_S):\n",
    "\n",
    "        # Convert sparse matrices to dense arrays if necessary\n",
    "        if issparse(X_S):\n",
    "            X_S = X_S.toarray()\n",
    "        if issparse(Y_S):\n",
    "            Y_S = Y_S.toarray()\n",
    "        \n",
    "        #Norm calculation X_S - Y_S\n",
    "        normXminusY = np.linalg.norm(X_S - Y_S, ord=2)\n",
    "\n",
    "        #Norm of Y_S\n",
    "        norm_Y = np.linalg.norm(Y_S, ord=2)\n",
    "\n",
    "        if norm_Y == 0:\n",
    "            raise ValueError(\"Norm of Y is 0\")\n",
    "\n",
    "        #Calculate the error\n",
    "        error = normXminusY/norm_Y\n",
    "    \n",
    "        return error\n",
    "\n",
    "    def _error_calc_validation(self, X, val_indices, val_values):\n",
    "        \n",
    "        # Extract predicted values at the validation indices\n",
    "        pred_values = np.array([X[i, j] for (i, j) in val_indices])\n",
    "\n",
    "        # Compute the Frobenius norm of the difference\n",
    "        numerator = np.linalg.norm(pred_values - val_values, ord=2)\n",
    "\n",
    "        # Compute the Frobenius norm of the validation values\n",
    "        denominator = np.linalg.norm(val_values, ord=2)\n",
    "\n",
    "        if denominator == 0:\n",
    "            raise ValueError(\"Norm of validation values is 0\")\n",
    "\n",
    "        # Compute the relative error\n",
    "        error = numerator / denominator\n",
    "\n",
    "        return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 35.117518 seconds with 3 iterations and error 0.064137.\n",
      "Fit time SVT One iteration:  11.705839236577352\n",
      "Fit time: 11.886038 seconds with 2 iterations and error 0.070169.\n",
      "Fit time: 11.927572 seconds with 2 iterations and error 0.072201.\n",
      "Fit time: 12.304155 seconds with 2 iterations and error 0.071113.\n",
      "Fit time: 16.400495 seconds with 3 iterations and error 0.069509.\n",
      "Fit time: 19.446574 seconds with 4 iterations and error 0.074855.\n",
      "Cross validation errors:  [0.93201281 0.92213269 0.93473676 0.93388739 0.93098753]\n",
      "Mean CV error:  0.9307514365661478\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Fit once for timing purposes\n",
    "svt_estimator = SVTEstimator(learning_rate=1, max_iter=100, tol=1e-3, tau=tau)\n",
    "svt_estimator.fit(sparse_matrix_train)\n",
    "SVTTimingOneIter = svt_estimator.fit_time/svt_estimator.iter\n",
    "print(\"Fit time SVT One iteration: \", SVTTimingOneIter)\n",
    "\n",
    "# Perform cross-validation\n",
    "\n",
    "svt_estimator = SVTEstimator(learning_rate=1, max_iter=100, tol=1e-3, tau=tau)\n",
    "   \n",
    "#Perform 5 fold CV\n",
    "cv_scores = cross_val_score(svt_estimator, sparse_matrix_train, cv=5, n_jobs=10)\n",
    "\n",
    "print(\"Cross validation errors: \", -cv_scores)\n",
    "print(\"Mean CV error: \", -np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve SVT via learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 563.936560 seconds with 31 iterations and error 0.109637.\n",
      "Fit time: 579.147510 seconds with 29 iterations and error 0.099491.\n",
      "Fit time: 596.240002 seconds with 29 iterations and error 0.099346.\n",
      "Fit time: 606.115481 seconds with 31 iterations and error 0.103978.\n",
      "Fit time: 616.166507 seconds with 32 iterations and error 0.099348.\n",
      "Cross validation errors:  [0.93941369 0.93967368 0.90919421 0.93241664 0.93610611]\n",
      "Mean CV error:  0.9313608659037327\n",
      "Fit time: 424.424214 seconds with 23 iterations and error 0.095932.\n",
      "Fit time: 442.844289 seconds with 22 iterations and error 0.087054.\n",
      "Fit time: 446.545746 seconds with 22 iterations and error 0.086928.\n",
      "Fit time: 455.662537 seconds with 23 iterations and error 0.090981.\n",
      "Fit time: 474.814241 seconds with 25 iterations and error 0.086930.\n",
      "Cross validation errors:  [0.93989508 0.93975547 0.90949782 0.93260845 0.93624331]\n",
      "Mean CV error:  0.9316000249196227\n",
      "Fit time: 304.901247 seconds with 16 iterations and error 0.085273.\n",
      "Fit time: 337.233125 seconds with 17 iterations and error 0.080872.\n",
      "Fit time: 351.613059 seconds with 17 iterations and error 0.077269.\n",
      "Fit time: 353.999189 seconds with 18 iterations and error 0.077271.\n",
      "Fit time: 356.957885 seconds with 18 iterations and error 0.077382.\n",
      "Cross validation errors:  [0.94058064 0.9400874  0.91011489 0.93307779 0.9366364 ]\n",
      "Mean CV error:  0.9320994227669811\n",
      "Fit time: 12.379019 seconds with 2 iterations and error 0.069544.\n",
      "Fit time: 12.843283 seconds with 2 iterations and error 0.072785.\n",
      "Fit time: 17.595152 seconds with 3 iterations and error 0.069643.\n",
      "Fit time: 17.876759 seconds with 3 iterations and error 0.069542.\n",
      "Fit time: 20.108005 seconds with 4 iterations and error 0.076746.\n",
      "Cross validation errors:  [0.94134136 0.94057205 0.91094696 0.93366177 0.93716691]\n",
      "Mean CV error:  0.9327378104635983\n",
      "Best leraning rate: 1\n",
      "Best error: 0.9313608659037327\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# Define a range for learning rates\n",
    "learning_rates = [0.7, 0.8, 0.9, 1]\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_errors = []\n",
    "for learning_rate in learning_rates:\n",
    "    svp_estimator = SVTEstimator(tau=tau, learning_rate=learning_rate, max_iter=100, tol=1e-3)\n",
    "    cv_scores = cross_val_score(svp_estimator, sparse_matrix_train, cv=KFold(n_splits=5, shuffle=True, random_state=42), n_jobs=10)\n",
    "    cv_errors.append(-np.mean(cv_scores))  # Convert scores to errors\n",
    "    print(\"Cross validation errors: \", -cv_scores)\n",
    "    print(\"Mean CV error: \", -np.mean(cv_scores))\n",
    "\n",
    "# Find the best lambda\n",
    "best_learning_rate = learning_rates[np.argmin(cv_errors)]\n",
    "print(f\"Best leraning rate: {learning_rate}\")\n",
    "\n",
    "# Best error\n",
    "best_error_svt = np.min(cv_errors)\n",
    "print(f\"Best error: {best_error_svt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion SVT\n",
    "* SVT provides soft thresholding to the singular values\n",
    "* This in essence is the nuclear norm regularization\n",
    "* It is less robust to noise\n",
    "* Convergence speed is slower\n",
    "* Performance can get worst in presense of outlier\n",
    "* $\\tau$ parameter tuning is very important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADMiRA (Atomic decomposition for Minimization Risk Approximation)\n",
    "ADMIRA is based on the idea of decomposing a signal or function into a combination of simpler, \"atomic\" components. These atoms are selected from a predefined dictionary (a set of basis functions or features) to best approximate the target signal. The goal is to achieve a sparse representation (using as few atoms as possible) while minimizing the approximation error or risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADMIRA is an extension of the compressed sensing matching pursuit algorithm, where it iteratively selects \"atoms\" (rank 1 matrices) to approximate the target matrix, making it particularly useful for matrix completion and robust PCA problems. The approach is a \"greedy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm flow is as follows:  \n",
    "Input the target rank.\n",
    "* 1. SVD the residual and select the top 2 x Rank atoms (rank 1 matrices)\n",
    "* 2. Add the selected atoms to the current approximation\n",
    "* 3. Truncate to retain the top rank atoms\n",
    "* 4. Update the residual (Y - X)\n",
    "* 5. Calculate the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix, csr_matrix, issparse\n",
    "from scipy.sparse.linalg import svds\n",
    "import time\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class ADMIRAEstimator(BaseEstimator):\n",
    "    def __init__(self, rank=10, max_iter=100, tol=1e-3):\n",
    "        self.rank = rank\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.X = None\n",
    "        self.fit_time = None\n",
    "        self.error = None\n",
    "        self.iter = None\n",
    "\n",
    "    def fit(self, Y, y=None):\n",
    "        start_time = time.time()\n",
    "        # Initialize the approximation matrix X\n",
    "        self.X = lil_matrix(Y.shape, dtype=np.float64)\n",
    "        residual = Y - self.X  # Initial residual\n",
    "        iter = 0\n",
    "        error = self.tol + 99\n",
    "        prev_error = 100\n",
    "\n",
    "        while error > self.tol and iter < self.max_iter and prev_error > error:\n",
    "            # Update the previous error\n",
    "            prev_error = error\n",
    "            \n",
    "            # Step 1: Select top 2 * rank atoms from the residual\n",
    "            U, S, Vt = svds(residual, k=2 * self.rank)\n",
    "            U = csr_matrix(U)\n",
    "            Vt = csr_matrix(Vt)\n",
    "            S = csr_matrix(np.diag(S))\n",
    "\n",
    "            # Step 2: Combine the selected atoms with the current approximation\n",
    "            X_candidate = self.X + U @ S @ Vt\n",
    "\n",
    "            # Step 3: Truncate to retain only the top rank atoms\n",
    "            U_trunc, S_trunc, Vt_trunc = svds(X_candidate, k=self.rank)\n",
    "            U_trunc = csr_matrix(U_trunc)\n",
    "            Vt_trunc = csr_matrix(Vt_trunc)\n",
    "            S_trunc = csr_matrix(np.diag(S_trunc))\n",
    "            self.X = U_trunc @ S_trunc @ Vt_trunc\n",
    "\n",
    "            # Step 4: Update the residual\n",
    "            residual = Y - self.X\n",
    "\n",
    "            # Step 5: Calculate the error\n",
    "            error = self._error_calc(residual, Y)\n",
    "            iter += 1\n",
    "\n",
    "        self.error = error\n",
    "        self.iter = iter\n",
    "        self.fit_time = time.time() - start_time\n",
    "        print(f\"Fit time: {self.fit_time:.6f} seconds with {self.iter} iterations and error {self.error:.6f}.\")\n",
    "        return self\n",
    "\n",
    "    def score(self, Y, y=None):\n",
    "        # Extract observed entries in the validation fold\n",
    "        val_indices = list(zip(Y.nonzero()[0], Y.nonzero()[1]))\n",
    "        val_values = Y.data\n",
    "\n",
    "        # Compute the error only on the observed entries in the validation fold\n",
    "        error = self._error_calc_validation(self.X, val_indices, val_values)\n",
    "        return -error  # For scikit-learn, higher score is better\n",
    "\n",
    "    def _error_calc(self, residual, Y):\n",
    "        # Convert sparse matrices to dense arrays if necessary\n",
    "        if issparse(residual):\n",
    "            residual = residual.toarray()\n",
    "        if issparse(Y):\n",
    "            Y = Y.toarray()\n",
    "\n",
    "        # Norm calculation residual\n",
    "        norm_residual = np.linalg.norm(residual, ord=2)\n",
    "\n",
    "        # Norm of Y\n",
    "        norm_Y = np.linalg.norm(Y, ord=2)\n",
    "\n",
    "        if norm_Y == 0:\n",
    "            raise ValueError(\"Norm of Y is 0\")\n",
    "\n",
    "        # Calculate the error\n",
    "        error = norm_residual / norm_Y\n",
    "\n",
    "        return error\n",
    "\n",
    "    def _error_calc_validation(self, X, val_indices, val_values):\n",
    "        # Extract predicted values at the validation indices\n",
    "        pred_values = np.array([X[i, j] for (i, j) in val_indices])\n",
    "\n",
    "        # Compute the Frobenius norm of the difference\n",
    "        numerator = np.linalg.norm(pred_values - val_values, ord=2)\n",
    "\n",
    "        # Compute the Frobenius norm of the validation values\n",
    "        denominator = np.linalg.norm(val_values, ord=2)\n",
    "\n",
    "        if denominator == 0:\n",
    "            raise ValueError(\"Norm of validation values is 0\")\n",
    "\n",
    "        # Compute the relative error\n",
    "        error = numerator / denominator\n",
    "\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 57.088973 seconds with 2 iterations and error 0.064134.\n",
      "Fit time ADMIRA One iteration:  28.54448652267456\n",
      "Fit time: 59.380467 seconds with 2 iterations and error 0.063242.\n",
      "Fit time: 60.474447 seconds with 2 iterations and error 0.058039.\n",
      "Fit time: 61.567287 seconds with 2 iterations and error 0.060150.\n",
      "Fit time: 62.456772 seconds with 2 iterations and error 0.059354.\n",
      "Fit time: 79.724009 seconds with 3 iterations and error 0.060736.\n",
      "Cross validation errors:  [0.949641   0.94135507 0.95269472 0.95222441 0.95165312]\n",
      "Mean CV error:  0.9495136629343817\n"
     ]
    }
   ],
   "source": [
    "admira = ADMIRAEstimator(rank=rank, max_iter=100, tol=1e-3)\n",
    "\n",
    "#fit the model once for timing purposes\n",
    "admira.fit(sparse_matrix_train)\n",
    "ADMIRATimingOneIter = admira.fit_time/admira.iter\n",
    "print(\"Fit time ADMIRA One iteration: \", ADMIRATimingOneIter)\n",
    "\n",
    "#Perform 5 fold CV\n",
    "cv_scores = cross_val_score(admira, sparse_matrix_train, cv=5, n_jobs=10)\n",
    "\n",
    "print(\"Cross validation errors: \", -cv_scores)\n",
    "print(\"Mean CV error: \", -np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative errors of the 3 algorithms are very close (in the range of $0.05 \\tilde 0.07$). In terms of computation time the SVT takes the longest to execute. SVP is the fastest followed by ADMiRA.\n",
    "\n",
    "### Performance and Convergence\n",
    "SVP is simpler to analyze and implement than other methods and has a strong geometric convergence rate, making it faster. ADMiRA has a performance guarantee for the general case where the solution is approximately low-rank and the measurements are noisy. SVT is sensitive to noise and outliers and the convergence is slow.\n",
    "### Computational Complexity and Scalability\n",
    "SVP has low computational demands compared to other methods. For this dataset SVT takes longer than SVP. SVT usually becomes prohibitively expensive for moderately large datasets. ADMiRA also took a longer to execute compared to SVP.\n",
    "\n",
    "### Choice of algorithm\n",
    "SVP seems like the best choice here since the rank of the matrix is fixed in the beginning. SVP is based on iterative hard-thresholding. The algorithm involves projecting candidate solutions onto the set of low-rank matrices and has a geometric convergence rate even with noisy measurements. SVT on the other hand is sensitive to outliers and is too slow to converge. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
