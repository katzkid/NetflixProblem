{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: timestamping does nothing in combination with -O. See the manual\n",
      "for details.\n",
      "\n",
      "--2025-01-27 12:31:21--  https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 978202 (955K) [application/zip]\n",
      "Saving to: ‘ml-latest-small.zip’\n",
      "\n",
      "ml-latest-small.zip 100%[===================>] 955.28K  1.52MB/s    in 0.6s    \n",
      "\n",
      "2025-01-27 12:31:22 (1.52 MB/s) - ‘ml-latest-small.zip’ saved [978202/978202]\n",
      "\n",
      "Archive:  ml-latest-small.zip\n",
      "  inflating: ml-latest-small/links.csv  \n",
      "  inflating: ml-latest-small/tags.csv  \n",
      "  inflating: ml-latest-small/ratings.csv  \n",
      "  inflating: ml-latest-small/README.txt  \n",
      "  inflating: ml-latest-small/movies.csv  \n"
     ]
    }
   ],
   "source": [
    "!wget -O ml-latest-small.zip -N https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "!unzip -o ml-latest-small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ther ratings.csv file into a pandas dataframe\n",
    "import pandas as pd\n",
    "ratings = pd.read_csv('ml-latest-small/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100836, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Representation\n",
    "* Since the data is sparse (most users have not rated most movies), we should represent the data in a sparse format to save memory and improve computational efficiency.\n",
    "* The sparse format typically represents the data as a user-item matrix, where rows correspond to users, columns correspond to movies, and the values are the ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "* UserID and MovieID are categorical features and should be encoded using one-hot encoding or LabelEncoding\n",
    "* Rating is the target variable (output) we want to predict\n",
    "* Timestamp can be dropped as it won't contain any information regarding the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ratings data:\n",
      "   userId  movieId  rating  timestamp  user  movie\n",
      "0       1        1     4.0  964982703     0      0\n",
      "1       1        3     4.0  964981247     0      2\n",
      "2       1        6     4.0  964982224     0      5\n",
      "3       1       47     5.0  964983815     0     43\n",
      "4       1       50     5.0  964982931     0     46\n"
     ]
    }
   ],
   "source": [
    "# Label encode the user and movie ids\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoders\n",
    "user_enc = LabelEncoder()\n",
    "movie_enc = LabelEncoder()\n",
    "\n",
    "# Fit and transform the training data\n",
    "ratings['user'] = user_enc.fit_transform(ratings['userId'].values)\n",
    "ratings['movie'] = movie_enc.fit_transform(ratings['movieId'].values)\n",
    "\n",
    "# Display the first few rows of the test data\n",
    "print(\"\\nRatings data:\")\n",
    "print(ratings.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610 9724\n",
      "(610, 9724)\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 100836 stored elements and shape (610, 9724)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t4.0\n",
      "  (0, 2)\t4.0\n",
      "  (0, 5)\t4.0\n",
      "  (0, 43)\t5.0\n",
      "  (0, 46)\t5.0\n",
      "  (0, 62)\t3.0\n",
      "  (0, 89)\t5.0\n",
      "  (0, 97)\t4.0\n",
      "  (0, 124)\t5.0\n",
      "  (0, 130)\t5.0\n",
      "  (0, 136)\t5.0\n",
      "  (0, 184)\t5.0\n",
      "  (0, 190)\t3.0\n",
      "  (0, 197)\t5.0\n",
      "  (0, 201)\t4.0\n",
      "  (0, 224)\t5.0\n",
      "  (0, 257)\t3.0\n",
      "  (0, 275)\t3.0\n",
      "  (0, 291)\t5.0\n",
      "  (0, 307)\t4.0\n",
      "  (0, 314)\t4.0\n",
      "  (0, 320)\t5.0\n",
      "  (0, 325)\t4.0\n",
      "  (0, 367)\t3.0\n",
      "  (0, 384)\t4.0\n",
      "  :\t:\n",
      "  (609, 9238)\t5.0\n",
      "  (609, 9246)\t4.5\n",
      "  (609, 9256)\t4.0\n",
      "  (609, 9268)\t5.0\n",
      "  (609, 9274)\t3.5\n",
      "  (609, 9279)\t3.5\n",
      "  (609, 9282)\t3.0\n",
      "  (609, 9288)\t3.0\n",
      "  (609, 9304)\t3.0\n",
      "  (609, 9307)\t2.5\n",
      "  (609, 9312)\t4.5\n",
      "  (609, 9317)\t3.0\n",
      "  (609, 9324)\t3.0\n",
      "  (609, 9339)\t4.0\n",
      "  (609, 9341)\t4.0\n",
      "  (609, 9348)\t3.5\n",
      "  (609, 9371)\t3.5\n",
      "  (609, 9372)\t3.5\n",
      "  (609, 9374)\t5.0\n",
      "  (609, 9415)\t4.0\n",
      "  (609, 9416)\t4.0\n",
      "  (609, 9443)\t5.0\n",
      "  (609, 9444)\t5.0\n",
      "  (609, 9445)\t5.0\n",
      "  (609, 9485)\t3.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "#create a sparse matrix of train data\n",
    "n_users = ratings['user'].max() + 1\n",
    "n_movies = ratings['movie'].max() + 1\n",
    "print(n_users, n_movies)\n",
    "\n",
    "sparse_matrix_train = csr_matrix((ratings['rating'], (ratings['user'], ratings['movie'])), shape=(n_users, n_movies))\n",
    "\n",
    "print(sparse_matrix_train.shape)\n",
    "print(sparse_matrix_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank of the users x movie sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def rank_calculator(sparse_matrix_train, energy):\n",
    "  \n",
    "  #Convert to array\n",
    "  train_data_matrix = sparse_matrix_train.toarray()\n",
    "\n",
    "  #Perform SVD\n",
    "  u, s, vt = svds(train_data_matrix, k=min(train_data_matrix.shape) - 1)\n",
    "\n",
    "  #Sort the s values in descending order\n",
    "  s = np.sort(s)[::-1]\n",
    "\n",
    "  #Calculate the total energy\n",
    "  total_energy = np.sum(s**2)\n",
    "  current_energy = 0\n",
    "  rank = 0\n",
    "  for i in range(len(s)):\n",
    "    current_energy += s[i]**2\n",
    "    if current_energy/total_energy >= energy:\n",
    "      rank = i\n",
    "      break\n",
    "    \n",
    "  print(\"Rank of the matrix with {energy} energy: \", rank)\n",
    "  return rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the error calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse\n",
    "\n",
    "# error calculation function for sparse matrix\n",
    "\n",
    "def error_calc(X_S, Y_S):\n",
    "\n",
    "    # Convert sparse matrices to dense arrays if necessary\n",
    "    if issparse(X_S):\n",
    "        X_S = X_S.toarray()\n",
    "    if issparse(Y_S):\n",
    "        Y_S = Y_S.toarray()\n",
    "    \n",
    "    #Norm calculation X_S - Y_S\n",
    "    normXminusY = np.linalg.norm(X_S - Y_S, ord=2)\n",
    "\n",
    "    #Norm of Y_S\n",
    "    norm_Y = np.linalg.norm(Y_S, ord=2)\n",
    "\n",
    "    if norm_Y == 0:\n",
    "        raise ValueError(\"Norm of Y is 0\")\n",
    "\n",
    "    #Calculate the error\n",
    "    error = normXminusY/norm_Y\n",
    "  \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Projection Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVP aims to find a low-rank matrix $X$ taht approximates an observed matrix $Y$ by solving:\n",
    "$$\n",
    "\\min_{X} \\|X - Y\\|_F^2 \\quad \\text{subject to} \\quad \\text{rank}(X) \\leq r\n",
    "$$\n",
    "where the rank $r$ is a fixed desired rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of the matrix with {energy} energy:  239\n"
     ]
    }
   ],
   "source": [
    "# Get the rank for 90% energy\n",
    "rank = rank_calculator(sparse_matrix_train, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVP Estimator class to use with cross validation in scikit learn\n",
    "\n",
    "class SVPEstimator(BaseEstimator):\n",
    "    def __init__(self, rank=10, max_iter=100, tol=1e-3, learning_rate=1):\n",
    "        self.rank = rank\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.learning_rate = learning_rate\n",
    "        self.X = None\n",
    "        self.fit_time = None\n",
    "        self.error = None\n",
    "        self.iter = None\n",
    "        \n",
    "\n",
    "    def fit(self, Y, y=None):\n",
    "        start_time = time.time()\n",
    "        #Init X\n",
    "        self.X = lil_matrix(Y.shape, dtype = np.float64) #Sparse matrix with 64 bit float values for better stability\n",
    "        iter = 0\n",
    "        error = self.tol + 99\n",
    "        prev_error = 100\n",
    "        while(error > self.tol and iter < self.max_iter and prev_error > error):\n",
    "            #Update the previous error\n",
    "            prev_error = error\n",
    "\n",
    "            #Update the iteration count\n",
    "            iter += 1\n",
    "\n",
    "            #Update the X matrix\n",
    "            X_half = self.X + self.learning_rate * (Y - self.X)\n",
    "            U, S, Vt = svds(X_half, k = self.rank)\n",
    "            U_sparse = csr_matrix(U)\n",
    "            Vt_sparse = csr_matrix(Vt)\n",
    "            S_sparse = csr_matrix(np.diag(S))\n",
    "            self.X = U_sparse @ S_sparse @ Vt_sparse \n",
    "            \n",
    "            #Calculate the error\n",
    "            error = self._error_calc(self.X, Y)\n",
    "\n",
    "        self.error = error\n",
    "        self.iter = iter\n",
    "        self.fit_time = time.time() - start_time\n",
    "        print(f\"Fit time: {self.fit_time:.6f} seconds with {self.iter} iterations and error {self.error:.6f}.\")\n",
    "        return self\n",
    "    \n",
    "    def score(self, Y, y=None):\n",
    "        # Extract observed entries in the validation fold\n",
    "        val_indices = list(zip(Y.nonzero()[0], Y.nonzero()[1]))\n",
    "        val_values = Y.data\n",
    "\n",
    "        # Compute the error only on the observed entries in the validation fold\n",
    "        error = self._error_calc_validation(self.X, val_indices, val_values)\n",
    "        return -error  # For scikit-learn, higher score is better\n",
    "\n",
    "    def _error_calc(self, X_S, Y_S):\n",
    "\n",
    "        # Convert sparse matrices to dense arrays if necessary\n",
    "        if issparse(X_S):\n",
    "            X_S = X_S.toarray()\n",
    "        if issparse(Y_S):\n",
    "            Y_S = Y_S.toarray()\n",
    "        \n",
    "        #Norm calculation X_S - Y_S\n",
    "        normXminusY = np.linalg.norm(X_S - Y_S, ord=2)\n",
    "\n",
    "        #Norm of Y_S\n",
    "        norm_Y = np.linalg.norm(Y_S, ord=2)\n",
    "\n",
    "        if norm_Y == 0:\n",
    "            raise ValueError(\"Norm of Y is 0\")\n",
    "\n",
    "        #Calculate the error\n",
    "        error = normXminusY/norm_Y\n",
    "    \n",
    "        return error\n",
    "\n",
    "    def _error_calc_validation(self, X, val_indices, val_values):\n",
    "        \n",
    "        # Extract predicted values at the validation indices\n",
    "        pred_values = np.array([X[i, j] for (i, j) in val_indices])\n",
    "\n",
    "        # Compute the Frobenius norm of the difference\n",
    "        numerator = np.linalg.norm(pred_values - val_values, ord=2)\n",
    "\n",
    "        # Compute the Frobenius norm of the validation values\n",
    "        denominator = np.linalg.norm(val_values, ord=2)\n",
    "\n",
    "        if denominator == 0:\n",
    "            raise ValueError(\"Norm of validation values is 0\")\n",
    "\n",
    "        # Compute the relative error\n",
    "        error = numerator / denominator\n",
    "\n",
    "        return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 12.786731 seconds with 2 iterations and error 0.058972.\n",
      "Fit time SVP One iteration:  6.393365502357483\n",
      "Fit time: 9.587636 seconds with 2 iterations and error 0.052819.\n",
      "Fit time: 14.999391 seconds with 3 iterations and error 0.054237.\n",
      "Fit time: 14.557200 seconds with 3 iterations and error 0.055145.\n",
      "Fit time: 9.556632 seconds with 2 iterations and error 0.057840.\n",
      "Fit time: 9.384391 seconds with 2 iterations and error 0.055710.\n",
      "Cross validation errors:  [0.95114829 0.9419772  0.95355127 0.95305303 0.95217684]\n",
      "Mean CV error:  0.950381327654785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#Initialize the SVP Estimator\n",
    "svp_estimator = SVPEstimator(rank=rank, max_iter=100, tol=1e-3, learning_rate=1)\n",
    "\n",
    "#fit the model once for timing purposes\n",
    "svp_estimator.fit(sparse_matrix_train)\n",
    "SVPTimingOneIter = svp_estimator.fit_time/svp_estimator.iter\n",
    "print(\"Fit time SVP One iteration: \", SVPTimingOneIter)\n",
    "\n",
    "#Perform 5 fold CV\n",
    "cv_scores = cross_val_score(svp_estimator, sparse_matrix_train, cv=5)\n",
    "\n",
    "print(\"Cross validation errors: \", -cv_scores)\n",
    "print(\"Mean CV error: \", -np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve SVP\n",
    "* Lets try to improve the algorithm by specifying a learning rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 462.462032 seconds with 29 iterations and error 0.055710.\n",
      "Fit time: 475.604428 seconds with 30 iterations and error 0.057840.\n",
      "Fit time: 486.367984 seconds with 29 iterations and error 0.055145.\n",
      "Fit time: 500.586351 seconds with 31 iterations and error 0.052819.\n",
      "Fit time: 500.770495 seconds with 30 iterations and error 0.054237.\n",
      "Cross validation errors:  [0.95124582 0.94207106 0.9536366  0.95300789 0.95199927]\n",
      "Mean CV error:  0.9503921279103466\n"
     ]
    }
   ],
   "source": [
    "#Initialize the SVP Estimator\n",
    "svp_estimator = SVPEstimator(rank=rank, max_iter=100, tol=1e-3, learning_rate=0.1)\n",
    "\n",
    "#Perform 5 fold CV\n",
    "cv_scores = cross_val_score(svp_estimator, sparse_matrix_train, cv=5, n_jobs=10)\n",
    "\n",
    "print(\"Cross validation errors: \", -cv_scores)\n",
    "print(\"Mean CV error: \", -np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion SVP\n",
    "* The SVP algorithm converges quite fast since the rank is fixed. (Hard rank constraint)\n",
    "* There seems to be over-fitting happening looking at the cross validation error scores. This means that it is capturing the noise in the training folds. Probably the rank of the matrix is chosen to be too high.\n",
    "* This SVP method does not have regularization. Maybe with regularization the results could be better, as we can penelize large values. (As we will see in the next algorithm)\n",
    "* Increasing the data could also yield better results.\n",
    "*  By adding the learning rate parameter of 0.1 the results are identical, which is expected, since in all cases the maximum number of iterations is not acheived, and we are at the same minimum point on the convex curve.\n",
    "* The SVP algorithm is best suited for data that has less noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVT introduces a soft thresholding of the singular values which brings about a nuclear norm regularization.\n",
    "$$\n",
    "\\min_{X} \\|X - Y\\|_F^2 + \\lambda \\|X\\|_*\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVP Estimator class to use with cross validation in scikit learn\n",
    "\n",
    "class SVTEstimator(BaseEstimator):\n",
    "    def __init__(self, rank=10, max_iter=100, tol=1e-3, learning_rate=1, lambda_=0):\n",
    "        self.rank = rank\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_ = lambda_\n",
    "        self.X = None\n",
    "        self.fit_time = None\n",
    "        self.error = None\n",
    "        self.iter = None\n",
    "        \n",
    "\n",
    "    def fit(self, Y, y=None):\n",
    "        start_time = time.time()\n",
    "        #Init X\n",
    "        self.X = lil_matrix(Y.shape, dtype = np.float64) #Sparse matrix with 64 bit float values for better stability\n",
    "        iter = 0\n",
    "        error = self.tol + 99\n",
    "        prev_error = 100\n",
    "        while(error > self.tol and iter < self.max_iter and prev_error > error):\n",
    "            #Update the previous error\n",
    "            prev_error = error\n",
    "\n",
    "            #Update the iteration count\n",
    "            iter += 1\n",
    "\n",
    "            #Update the X matrix\n",
    "            X_half = self.X + self.learning_rate * (Y - self.X)\n",
    "            U, S, Vt = svds(X_half, k = min(X_half.shape) - 1) # Take full possible rank\n",
    "            S = np.maximum(S - self.lambda_, 0) #Soft thresholding\n",
    "            U_sparse = csr_matrix(U)\n",
    "            Vt_sparse = csr_matrix(Vt)\n",
    "            S_sparse = csr_matrix(np.diag(S))\n",
    "            self.X = U_sparse @ S_sparse @ Vt_sparse \n",
    "\n",
    "            #Calculate the error\n",
    "            error = self._error_calc(self.X, Y)\n",
    "\n",
    "        self.error = error\n",
    "        self.iter = iter\n",
    "        self.fit_time = time.time() - start_time\n",
    "        print(f\"Fit time: {self.fit_time:.6f} seconds with {self.iter} iterations and error {self.error:.6f}.\")\n",
    "        return self\n",
    "    \n",
    "    def score(self, Y, y=None):\n",
    "        # Extract observed entries in the validation fold\n",
    "        val_indices = list(zip(Y.nonzero()[0], Y.nonzero()[1]))\n",
    "        val_values = Y.data\n",
    "\n",
    "        # Compute the error only on the observed entries in the validation fold\n",
    "        error = self._error_calc_validation(self.X, val_indices, val_values)\n",
    "        return -error  # For scikit-learn, higher score is better\n",
    "\n",
    "    def _error_calc(self, X_S, Y_S):\n",
    "\n",
    "        # Convert sparse matrices to dense arrays if necessary\n",
    "        if issparse(X_S):\n",
    "            X_S = X_S.toarray()\n",
    "        if issparse(Y_S):\n",
    "            Y_S = Y_S.toarray()\n",
    "        \n",
    "        #Norm calculation X_S - Y_S\n",
    "        normXminusY = np.linalg.norm(X_S - Y_S, ord=2)\n",
    "\n",
    "        #Norm of Y_S\n",
    "        norm_Y = np.linalg.norm(Y_S, ord=2)\n",
    "\n",
    "        if norm_Y == 0:\n",
    "            raise ValueError(\"Norm of Y is 0\")\n",
    "\n",
    "        #Calculate the error\n",
    "        error = normXminusY/norm_Y\n",
    "    \n",
    "        return error\n",
    "\n",
    "    def _error_calc_validation(self, X, val_indices, val_values):\n",
    "        \n",
    "        # Extract predicted values at the validation indices\n",
    "        pred_values = np.array([X[i, j] for (i, j) in val_indices])\n",
    "\n",
    "        # Compute the Frobenius norm of the difference\n",
    "        numerator = np.linalg.norm(pred_values - val_values, ord=2)\n",
    "\n",
    "        # Compute the Frobenius norm of the validation values\n",
    "        denominator = np.linalg.norm(val_values, ord=2)\n",
    "\n",
    "        if denominator == 0:\n",
    "            raise ValueError(\"Norm of validation values is 0\")\n",
    "\n",
    "        # Compute the relative error\n",
    "        error = numerator / denominator\n",
    "\n",
    "        return error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of hyper parameter $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to select the correct values for the hyper parameter lambda.\n",
    "* $\\lambda$ determines the threshold for the singular values\n",
    "* Larger $\\lambda$ values shrink more singular values towards zero, resulting in lower-rank matrices (under-fitting)\n",
    "* Smaller $\\lambda$ values do the opposite, resulting in better fit for the training data (over-fitting)\n",
    "* Using the right $\\lambda$ we can control the over/under fitting of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 37.996586 seconds with 3 iterations and error 0.018712.\n",
      "Fit time SVT One iteration:  12.665528535842896\n",
      "Fit time: 18.509658 seconds with 2 iterations and error 0.008035.\n",
      "Fit time: 18.689390 seconds with 2 iterations and error 0.008388.\n",
      "Fit time: 18.958544 seconds with 2 iterations and error 0.007583.\n",
      "Fit time: 24.393835 seconds with 3 iterations and error 0.007786.\n",
      "Fit time: 26.080861 seconds with 3 iterations and error 0.007186.\n",
      "Cross validation errors:  [0.96113299 0.96012607 0.937755   0.95479565 0.95684214]\n",
      "Mean CV error:  0.9541303688695374\n",
      "Fit time: 17.167175 seconds with 2 iterations and error 0.007786.\n",
      "Fit time: 18.520374 seconds with 2 iterations and error 0.007186.Fit time: 18.501004 seconds with 2 iterations and error 0.008388.\n",
      "\n",
      "Fit time: 32.532050 seconds with 4 iterations and error 0.008035.\n",
      "Fit time: 33.822809 seconds with 4 iterations and error 0.007583.\n",
      "Cross validation errors:  [0.96112345 0.96011541 0.93774133 0.95478493 0.95683224]\n",
      "Mean CV error:  0.954119471654915\n",
      "Fit time: 18.253935 seconds with 2 iterations and error 0.007186.\n",
      "Fit time: 18.368393 seconds with 2 iterations and error 0.008035.\n",
      "Fit time: 18.458424 seconds with 2 iterations and error 0.008388.\n",
      "Fit time: 19.024314 seconds with 2 iterations and error 0.007583.\n",
      "Fit time: 24.938441 seconds with 3 iterations and error 0.007786.\n",
      "Cross validation errors:  [0.96102828 0.96000896 0.93760485 0.95467788 0.9567334 ]\n",
      "Mean CV error:  0.954010675141012\n",
      "Fit time: 18.906193 seconds with 2 iterations and error 0.007186.\n",
      "Fit time: 18.971791 seconds with 2 iterations and error 0.008388.\n",
      "Fit time: 19.130349 seconds with 2 iterations and error 0.008035.\n",
      "Fit time: 25.496994 seconds with 3 iterations and error 0.007786.\n",
      "Fit time: 27.026640 seconds with 3 iterations and error 0.007583.\n",
      "Cross validation errors:  [0.9600912  0.95896314 0.93626176 0.9536245  0.95576088]\n",
      "Mean CV error:  0.9529402957731306\n",
      "Fit time: 19.314515 seconds with 2 iterations and error 0.020289.\n",
      "Fit time: 25.182391 seconds with 3 iterations and error 0.022390.\n",
      "Fit time: 26.646564 seconds with 3 iterations and error 0.021235.\n",
      "Fit time: 26.670260 seconds with 3 iterations and error 0.020289.\n",
      "Fit time: 32.910904 seconds with 4 iterations and error 0.020318.\n",
      "Cross validation errors:  [0.95218936 0.95040668 0.92500616 0.94478743 0.94759485]\n",
      "Mean CV error:  0.9439968970959093\n",
      "Best lambda: 10.0\n",
      "Best error: 0.9439968970959093\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Fit once for timing purposes\n",
    "svt_estimator = SVTEstimator(lambda_=10, learning_rate=1, max_iter=100, tol=1e-3)\n",
    "svt_estimator.fit(sparse_matrix_train)\n",
    "SVTTimingOneIter = svt_estimator.fit_time/svt_estimator.iter\n",
    "print(\"Fit time SVT One iteration: \", SVTTimingOneIter)\n",
    "\n",
    "# Define a range for lambda\n",
    "lambda_values = np.logspace(-3, 1, 5)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_errors = []\n",
    "for lambda_ in lambda_values:\n",
    "    svt_estimator = SVTEstimator(lambda_=lambda_, learning_rate=1, max_iter=100, tol=1e-3)\n",
    "    cv_scores = cross_val_score(svt_estimator, sparse_matrix_train, cv=KFold(n_splits=5, shuffle=True, random_state=42), n_jobs=10)\n",
    "    cv_errors.append(-np.mean(cv_scores))  # Convert scores to errors\n",
    "    print(\"Cross validation errors: \", -cv_scores)\n",
    "    print(\"Mean CV error: \", -np.mean(cv_scores))\n",
    "\n",
    "# Find the best lambda\n",
    "best_lambda = lambda_values[np.argmin(cv_errors)]\n",
    "print(f\"Best lambda: {best_lambda}\")\n",
    "\n",
    "# Best error\n",
    "best_error_svt = np.min(cv_errors)\n",
    "print(f\"Best error: {best_error_svt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion SVT\n",
    "* SVT provides soft thresholding to the singular values\n",
    "* This in essence is the nuclear norm regularization\n",
    "* It is more robust to noise\n",
    "* Convergense speed is much slower\n",
    "* Useful when data is noisy\n",
    "* Lambda parameter tuning is very important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADMiRA (Atomic decomposition for Minimization Risk Approximation)\n",
    "ADMIRA is based on the idea of decomposing a signal or function into a combination of simpler, \"atomic\" components. These atoms are selected from a predefined dictionary (a set of basis functions or features) to best approximate the target signal. The goal is to achieve a sparse representation (using as few atoms as possible) while minimizing the approximation error or risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADMIRA is an extension of the compressed sensing matching pursuit algorithm, where it iteratively selects \"atoms\" (rank 1 matrices) to approximate the target matrix, making it particularly useful for matrix completion and robust PCA problems. The approach is a \"greedy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm flow is as follows:  \n",
    "Input the target rank.\n",
    "* 1. SVD the residual and select the top 2 x Rank atoms (rank 1 matrices)\n",
    "* 2. Add the selected atoms to the current approximation\n",
    "* 3. Truncate to retain the top rank atoms\n",
    "* 4. Update the residual (Y - X)\n",
    "* 5. Calculate the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix, csr_matrix, issparse\n",
    "from scipy.sparse.linalg import svds\n",
    "import time\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class ADMIRAEstimator(BaseEstimator):\n",
    "    def __init__(self, rank=10, max_iter=100, tol=1e-3):\n",
    "        self.rank = rank\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.X = None\n",
    "        self.fit_time = None\n",
    "        self.error = None\n",
    "        self.iter = None\n",
    "\n",
    "    def fit(self, Y, y=None):\n",
    "        start_time = time.time()\n",
    "        # Initialize the approximation matrix X\n",
    "        self.X = lil_matrix(Y.shape, dtype=np.float64)\n",
    "        residual = Y - self.X  # Initial residual\n",
    "        iter = 0\n",
    "        error = self.tol + 99\n",
    "        prev_error = 100\n",
    "\n",
    "        while error > self.tol and iter < self.max_iter and prev_error > error:\n",
    "            # Update the previous error\n",
    "            prev_error = error\n",
    "            \n",
    "            # Step 1: Select top 2 * rank atoms from the residual\n",
    "            U, S, Vt = svds(residual, k=2 * self.rank)\n",
    "            U = csr_matrix(U)\n",
    "            Vt = csr_matrix(Vt)\n",
    "            S = csr_matrix(np.diag(S))\n",
    "\n",
    "            # Step 2: Combine the selected atoms with the current approximation\n",
    "            X_candidate = self.X + U @ S @ Vt\n",
    "\n",
    "            # Step 3: Truncate to retain only the top rank atoms\n",
    "            U_trunc, S_trunc, Vt_trunc = svds(X_candidate, k=self.rank)\n",
    "            U_trunc = csr_matrix(U_trunc)\n",
    "            Vt_trunc = csr_matrix(Vt_trunc)\n",
    "            S_trunc = csr_matrix(np.diag(S_trunc))\n",
    "            self.X = U_trunc @ S_trunc @ Vt_trunc\n",
    "\n",
    "            # Step 4: Update the residual\n",
    "            residual = Y - self.X\n",
    "\n",
    "            # Step 5: Calculate the error\n",
    "            error = self._error_calc(residual, Y)\n",
    "            iter += 1\n",
    "\n",
    "        self.error = error\n",
    "        self.iter = iter\n",
    "        self.fit_time = time.time() - start_time\n",
    "        print(f\"Fit time: {self.fit_time:.6f} seconds with {self.iter} iterations and error {self.error:.6f}.\")\n",
    "        return self\n",
    "\n",
    "    def score(self, Y, y=None):\n",
    "        # Extract observed entries in the validation fold\n",
    "        val_indices = list(zip(Y.nonzero()[0], Y.nonzero()[1]))\n",
    "        val_values = Y.data\n",
    "\n",
    "        # Compute the error only on the observed entries in the validation fold\n",
    "        error = self._error_calc_validation(self.X, val_indices, val_values)\n",
    "        return -error  # For scikit-learn, higher score is better\n",
    "\n",
    "    def _error_calc(self, residual, Y):\n",
    "        # Convert sparse matrices to dense arrays if necessary\n",
    "        if issparse(residual):\n",
    "            residual = residual.toarray()\n",
    "        if issparse(Y):\n",
    "            Y = Y.toarray()\n",
    "\n",
    "        # Norm calculation residual\n",
    "        norm_residual = np.linalg.norm(residual, ord=2)\n",
    "\n",
    "        # Norm of Y\n",
    "        norm_Y = np.linalg.norm(Y, ord=2)\n",
    "\n",
    "        if norm_Y == 0:\n",
    "            raise ValueError(\"Norm of Y is 0\")\n",
    "\n",
    "        # Calculate the error\n",
    "        error = norm_residual / norm_Y\n",
    "\n",
    "        return error\n",
    "\n",
    "    def _error_calc_validation(self, X, val_indices, val_values):\n",
    "        # Extract predicted values at the validation indices\n",
    "        pred_values = np.array([X[i, j] for (i, j) in val_indices])\n",
    "\n",
    "        # Compute the Frobenius norm of the difference\n",
    "        numerator = np.linalg.norm(pred_values - val_values, ord=2)\n",
    "\n",
    "        # Compute the Frobenius norm of the validation values\n",
    "        denominator = np.linalg.norm(val_values, ord=2)\n",
    "\n",
    "        if denominator == 0:\n",
    "            raise ValueError(\"Norm of validation values is 0\")\n",
    "\n",
    "        # Compute the relative error\n",
    "        error = numerator / denominator\n",
    "\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 127.031344 seconds with 4 iterations and error 0.058972.\n",
      "Fit time ADMIRA One iteration:  31.757836043834686\n",
      "Fit time: 61.415307 seconds with 2 iterations and error 0.052819.\n",
      "Fit time: 63.879131 seconds with 2 iterations and error 0.055145.\n",
      "Fit time: 85.789806 seconds with 3 iterations and error 0.057840.\n",
      "Fit time: 86.200922 seconds with 3 iterations and error 0.055710.\n",
      "Fit time: 90.464997 seconds with 3 iterations and error 0.054237.\n",
      "Cross validation errors:  [0.95114829 0.9419772  0.95355127 0.95305303 0.95217684]\n",
      "Mean CV error:  0.950381327654785\n"
     ]
    }
   ],
   "source": [
    "admira = ADMIRAEstimator(rank=rank, max_iter=100, tol=1e-3)\n",
    "\n",
    "#fit the model once for timing purposes\n",
    "admira.fit(sparse_matrix_train)\n",
    "ADMIRATimingOneIter = admira.fit_time/admira.iter\n",
    "print(\"Fit time ADMIRA One iteration: \", ADMIRATimingOneIter)\n",
    "\n",
    "#Perform 5 fold CV\n",
    "cv_scores = cross_val_score(admira, sparse_matrix_train, cv=5, n_jobs=10)\n",
    "\n",
    "print(\"Cross validation errors: \", -cv_scores)\n",
    "print(\"Mean CV error: \", -np.mean(cv_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLlabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
